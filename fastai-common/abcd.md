**질문**: 차원의 저주는 어떻습니까?[[38:16]](https://www.youtube.com/watch?t=38m16s&v=CzdWqFTmn0Y&feature=youtu.be) 여러분이 자주 듣는 두가지 개념 차원의 저주(curse of dimensionality)와 공짜 점심은 없다(No Free Lunch Theorem)가 있습니다.
그것들은 둘 다 대체로 무위미하고 기본적으로 어리석지만, 현장에 있는 많은 사람들이 그것을 알 뿐만 아니라 반대로 생각하므로 설명할 가치가 있습니다.
차원의 저주는 열이 많을수록 점점 더 비어 있는 공간이 생성된다는 생각입니다.
차원이 많을수록 모든 점이 그 공간의 가장자리에 더 많이 위치한다는 매혹적인 수학적 아이디어가 있습니다. 만약 사물이 무작위인 단일 차원일 경우에는 전체에 흩어져 있습니다.
다른 곳에서 만약 그것이 정사각형이라면 그것들이 중간에 있을 확률은 그것들이 어느 차원이 가장자리에 있을 수 없다는 것을 의미하므로 가장자리에 없을 가능성이 조금 더 적습니다.
각 차원을 추가하면 점이 적어도 한차원의 가장자리에 없을 확률이 곱셈적으로 낮아집니다. 따라서 높은 차원에서는 모든 것이 가장자리에 위치합니다.
이론적으로 그것이 의미하는 것은 점들 사의 거리가 훨씬 의미가 없다는 것이다
그래서 만약 우리가 그것이 중요하다고 가정한다면, 그것은 여러분이 많은 열을 가지고 있고 여러분이 신경 쓰지 않는 열을 제거하기 위해 조심하지 않고 그것들을 사용한다면, 그것이 작동하지 않을 것이라는 것을 암시할 것이다. 이것은 여러 가지 이유로 인해 사실이 아닌 것으로 밝혀졌다.

* 점들은 여전히 서로 다른 거리를 가지고 있습니다. 가장자리에 있기 때문에 서로 얼마나 멀리 떨어져 있는지에 따라 여전히 다르므로 이 지점은 저 지점보다 이 지점에서 더 유사합니다
* 따라서 k-nearest neighbors과 같은 것들은 이론가들이 주장한 것과는 달리 고차원에서 실제로 잘 작동합니다. 여기서 실제로 일어난 일은 90년대에 이론이 기계 학습을 대신했다는 것입니다.
이러한 지원 벡터 머신의 개념은 이론적으로 충분히 정당화되었고, 수학적으로 분석하기도 쉬웠습니다. 그리고 여러분은 그들에 대한 것들을 증명할 수 있습니다. 그리고 우리는 10년간의 실질적인 발전을 잃었습니다. 그리고 이 모든 이론들은 차원의 저주처럼 매우 유명해졌습니다.
요즘 기계 학습의 세계는 매우 경험적이 되었고, 실제로 많은 열에 모델을 만드는 것이 정말 효과가 있다는 것이 밝혀졌습니다.
* 공짜 점심은 없다(No Free Lunch Theorem)[[41:08]](https://www.youtube.com/watch?v=CzdWqFTmn0Y&t=2468s)- 그들의 주장은 어떤 데이터셋에도 잘 작동하는 유형의 모델이 없다는 것입니다.
수학적 의미에서, 정의상 임의의 데이터셋은 무작위이므로, 다른 접근 방식보다 더 유용하게 모든 가능한 무작위 데이터 셋을 볼 수 있는 방법을 없을 것이다.
실제로, 우리는 무작위가 아닌 데이터를 봅니다. 수학적으로 우리는 그것이 어떤 저차원 다양체에 위치한다고 말할 것이다. 그것은 일종의 인과 구조에 의해 만들어 졌습니다. 
여기에는 몇가지 관계가 있습니다, 사실 우리는 무작위 데이터셋을 사용하지 않기 때문에 실제로 여러분이 보고 있는 거의 모든 데이터셋은 다른 기술보다 훨씬 잘 작동하는 기술이 있습니다.  오늘날, 어떤 기술이 많은 시간 동안 효과가 있는지 연구하는 경험적 연구자들이 있습니다. 결정트리의 앙상블, 즉 무작위로 포레스트를 만드는 것은 아마도 가장 자주 맨 위에 있는 기술일 것입니다.
Fast.ai는 적절하게 전처리 하고 매개 변수를 설정하는 표준 방법을 제공합니다.

## Scikit-learn[[42:54]](https://www.youtube.com/watch?v=CzdWqFTmn0Y&t=2574s)
파이썬에서 가장 인기있고 중요한 기계학습 패키지이다.
그것은 모든부분에서 가장 좋지는 않다.(예를 들면 XGBoost가 Gradient Boosting Tree보다 좋다) 하지만 거의 모든 부분에서 꽤 좋다.
RandomForestRegressor — 회귀 분석기는 연속 변수를 예측하는 방법(예: 회귀 분석)입니다.
RandomForestClassifier — 분류자는 범주형 변수를 예측하는 방법입니다. (예: 분류)
Scikit-learn 의 모든 것은 같은 형태를 가지고 있습니다.
 머신러닝 모델의 객체 인스턴스 생성
 fit함수에 독립 변수(예측할 변수)와 종속 변수(예측할 변수)를 전달하며 콜한다.
 axis=1 은 열을 제거하는 것을 의미합니다.
 shift + tab은 Jupyter 노트북에서 함수의 파라미터 검사를 불러옵니다.
"list-like"는 파이썬에서 색인화할 수 있는 모든 것을 의미합니다.
위의 코드는 에러가 날것입니다. 
데이터 세트 내에 "Conventional" 값이 있으며, 해당 문자열을 사용하여 모델을 만드는 방법을 알지 못했습니다.
